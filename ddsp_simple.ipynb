{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import yaml\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from os import path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from effortless_config import Config\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from core import get_scheduler, multiscale_fft, safe_log, mean_std_loudness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from models import DDSP_signal_only, DDSP_with_features\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "class args(Config):\n",
    "    CONFIG = \"config.yaml\"\n",
    "\n",
    "args.parse_args(\"\")\n",
    "with open(args.CONFIG, \"r\") as config:\n",
    "    config = yaml.safe_load(config)\n",
    "\n",
    "ddsp_model = DDSP_with_features(**config[\"model\"]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets.dataset_signal import Dataset\n",
    "\n",
    "# dataset = Dataset(config)\n",
    "# batch_size = config[\"hyperparams\"][\"batch_size\"]\n",
    "# dataloader = torch.utils.data.DataLoader(dataset,\n",
    "#                                         batch_size,\n",
    "#                                         shuffle=False,\n",
    "#                                         drop_last=False,\n",
    "#                                         )\n",
    "\n",
    "# print(\"Size of dataset:\", len(dataset), \"\\nSize of sig batch:\", next(iter(dataloader)).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 81 \n",
      "Size of sig batch: torch.Size([16, 64000]) \n",
      "Size of sig batch: torch.Size([16, 400]) \n",
      "Size of sig batch: torch.Size([16, 400])\n"
     ]
    }
   ],
   "source": [
    "from datasets.dataset_all import Dataset, get_files\n",
    "from effortless_config import Config\n",
    "import yaml\n",
    "import torch\n",
    "\n",
    "#get_files(\"config.yaml\")\n",
    "\n",
    "class args(Config):\n",
    "    CONFIG = \"config.yaml\"\n",
    "\n",
    "args.parse_args(\"\")\n",
    "with open(args.CONFIG, \"r\") as config:\n",
    "    config = yaml.safe_load(config)\n",
    "\n",
    "out_dir = config[\"preprocess\"][\"out_dir\"]\n",
    "\n",
    "dataset = Dataset(out_dir)\n",
    "batch_size = config[\"hyperparams\"][\"batch_size\"]\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset,\n",
    "                                        batch_size,\n",
    "                                        True,\n",
    "                                        drop_last=True,\n",
    "                                        )\n",
    "\n",
    "print(\"Size of dataset:\", len(dataset), \"\\nSize of sig batch:\", next(iter(dataloader_train))['signals'].size(), \"\\nSize of sig batch:\", next(iter(dataloader_train))['pitches'].size(), \"\\nSize of sig batch:\", next(iter(dataloader_train))['loudness'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args(Config):\n",
    "    CONFIG = \"config.yaml\"\n",
    "    NAME = \"debug/male_speech\"\n",
    "    ROOT = \"runs\"\n",
    "    STEPS = 500000\n",
    "    START_LR = 1e-3\n",
    "    STOP_LR = 1e-4\n",
    "    DECAY_OVER = 400000\n",
    "\n",
    "mean_loudness, std_loudness = mean_std_loudness(dataloader_train)\n",
    "config[\"data\"][\"mean_loudness\"] = mean_loudness\n",
    "config[\"data\"][\"std_loudness\"] = std_loudness\n",
    "\n",
    "writer = SummaryWriter(path.join(args.ROOT, args.NAME), flush_secs=20)\n",
    "\n",
    "with open(path.join(args.ROOT, args.NAME, \"config.yaml\"), \"w\") as out_config:\n",
    "    yaml.safe_dump(config, out_config)\n",
    "\n",
    "opt = torch.optim.Adam(ddsp_model.parameters(), lr=args.START_LR)\n",
    "\n",
    "schedule = get_scheduler(\n",
    "    len(dataloader_train),\n",
    "    args.START_LR,\n",
    "    args.STOP_LR,\n",
    "    args.DECAY_OVER,\n",
    ")\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "mean_loss = 0\n",
    "n_element = 0\n",
    "step = 0\n",
    "epochs = int(np.ceil(args.STEPS / len(dataloader_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses = []\n",
    "\n",
    "# def train(model, loader, optimizer):\n",
    "#     model.train()\n",
    "#     device = next(model.parameters()).device\n",
    "#     total_loss = 0\n",
    "\n",
    "#     for batch in loader:\n",
    "#         batch = batch.to(device)\n",
    "#         y = ddsp_model(batch).squeeze(-1)\n",
    "                \n",
    "#         ori_stft = multiscale_fft(\n",
    "#             batch,\n",
    "#             config[\"train\"][\"scales\"],\n",
    "#             config[\"train\"][\"overlap\"],\n",
    "#         )\n",
    "#         rec_stft = multiscale_fft(\n",
    "#             y,\n",
    "#             config[\"train\"][\"scales\"],\n",
    "#             config[\"train\"][\"overlap\"],\n",
    "#         )\n",
    "\n",
    "#         loss = 0\n",
    "#         for s_x, s_y in zip(ori_stft, rec_stft):\n",
    "#             lin_loss = (s_x - s_y).abs().mean()\n",
    "#             log_loss = (safe_log(s_x) - safe_log(s_y)).abs().mean()\n",
    "#             loss = loss + lin_loss + log_loss\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     total_loss /= len(loader)\n",
    "#     losses.append(total_loss)\n",
    "    \n",
    "#     return total_loss\n",
    "\n",
    "# for e in tqdm(range(epochs)):\n",
    "#     loss = train(ddsp_model, dataloader, opt)\n",
    "#     print(\"Epoch {} -- Loss {:3E}\".format(e, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 5625/38462 [1:32:01<8:57:15,  1.02it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Student projects\\ML-AND\\DDSP-Experiments\\ddsp_simple.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Student%20projects/ML-AND/DDSP-Experiments/ddsp_simple.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m l \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mloudness\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Student%20projects/ML-AND/DDSP-Experiments/ddsp_simple.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m l \u001b[39m=\u001b[39m (l \u001b[39m-\u001b[39m mean_loudness) \u001b[39m/\u001b[39m std_loudness\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Student%20projects/ML-AND/DDSP-Experiments/ddsp_simple.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m y \u001b[39m=\u001b[39m ddsp_model(s, p, l)\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Student%20projects/ML-AND/DDSP-Experiments/ddsp_simple.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m ori_stft \u001b[39m=\u001b[39m multiscale_fft(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Student%20projects/ML-AND/DDSP-Experiments/ddsp_simple.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     s,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Student%20projects/ML-AND/DDSP-Experiments/ddsp_simple.ipynb#W6sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     config[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mscales\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Student%20projects/ML-AND/DDSP-Experiments/ddsp_simple.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     config[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39moverlap\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Student%20projects/ML-AND/DDSP-Experiments/ddsp_simple.ipynb#W6sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Student%20projects/ML-AND/DDSP-Experiments/ddsp_simple.ipynb#W6sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m rec_stft \u001b[39m=\u001b[39m multiscale_fft(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Student%20projects/ML-AND/DDSP-Experiments/ddsp_simple.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     y,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Student%20projects/ML-AND/DDSP-Experiments/ddsp_simple.ipynb#W6sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     config[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mscales\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Student%20projects/ML-AND/DDSP-Experiments/ddsp_simple.ipynb#W6sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     config[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39moverlap\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Student%20projects/ML-AND/DDSP-Experiments/ddsp_simple.ipynb#W6sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Me-Lab_Chimaera\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Student projects\\ML-AND\\DDSP-Experiments\\models.py:226\u001b[0m, in \u001b[0;36mDDSP_with_features.forward\u001b[1;34m(self, signal, pitch, loudness)\u001b[0m\n\u001b[0;32m    222\u001b[0m pitch \u001b[39m=\u001b[39m upsample(pitch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock_size)\n\u001b[0;32m    224\u001b[0m harmonic \u001b[39m=\u001b[39m harmonic_synth(pitch, amplitudes, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_rate)\n\u001b[1;32m--> 226\u001b[0m impulse \u001b[39m=\u001b[39m amp_to_impulse_response(param_noise, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblock_size)\n\u001b[0;32m    227\u001b[0m noise \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\n\u001b[0;32m    228\u001b[0m     impulse\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],\n\u001b[0;32m    229\u001b[0m     impulse\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m],\n\u001b[0;32m    230\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock_size,\n\u001b[0;32m    231\u001b[0m )\u001b[39m.\u001b[39mto(impulse) \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    233\u001b[0m noise \u001b[39m=\u001b[39m fft_convolve(noise, impulse)\u001b[39m.\u001b[39mcontiguous()\n",
      "File \u001b[1;32md:\\Student projects\\ML-AND\\DDSP-Experiments\\core.py:111\u001b[0m, in \u001b[0;36mamp_to_impulse_response\u001b[1;34m(amp, target_size)\u001b[0m\n\u001b[0;32m    107\u001b[0m win \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mhann_window(filter_size, dtype\u001b[39m=\u001b[39mamp\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39mamp\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    109\u001b[0m amp \u001b[39m=\u001b[39m amp \u001b[39m*\u001b[39m win\n\u001b[1;32m--> 111\u001b[0m amp \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mpad(amp, (\u001b[39m0\u001b[39m, \u001b[39mint\u001b[39;49m(target_size) \u001b[39m-\u001b[39m \u001b[39mint\u001b[39m(filter_size)))\n\u001b[0;32m    112\u001b[0m amp \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mroll(amp, \u001b[39m-\u001b[39mfilter_size \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    114\u001b[0m \u001b[39mreturn\u001b[39;00m amp\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "losses = []\n",
    "\n",
    "for e in tqdm(range(epochs)):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader_train:\n",
    "        s = batch['signals'].to(device)\n",
    "        p = batch['pitches'].unsqueeze(-1).to(device)\n",
    "        l = batch['loudness'].unsqueeze(-1).to(device)\n",
    "\n",
    "        l = (l - mean_loudness) / std_loudness\n",
    "\n",
    "        y = ddsp_model(s, p, l).squeeze(-1)\n",
    "\n",
    "        ori_stft = multiscale_fft(\n",
    "            s,\n",
    "            config[\"train\"][\"scales\"],\n",
    "            config[\"train\"][\"overlap\"],\n",
    "        )\n",
    "        rec_stft = multiscale_fft(\n",
    "            y,\n",
    "            config[\"train\"][\"scales\"],\n",
    "            config[\"train\"][\"overlap\"],\n",
    "        )\n",
    "\n",
    "        loss = 0\n",
    "        for s_x, s_y in zip(ori_stft, rec_stft):\n",
    "            lin_loss = (s_x - s_y).abs().mean()\n",
    "            log_loss = (safe_log(s_x) - safe_log(s_y)).abs().mean()\n",
    "            loss = loss + lin_loss + log_loss\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        writer.add_scalar(\"loss\", loss.item(), step)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "        n_element += 1\n",
    "        mean_loss += (loss.item() - mean_loss) / n_element\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if not e % 10:\n",
    "        writer.add_scalar(\"lr\", schedule(e), e)\n",
    "        writer.add_scalar(\"reverb_decay\", ddsp_model.reverb.decay.item(), e)\n",
    "        writer.add_scalar(\"reverb_wet\", ddsp_model.reverb.wet.item(), e)\n",
    "        # scheduler.step()\n",
    "        if mean_loss < best_loss:\n",
    "            best_loss = mean_loss\n",
    "            torch.save(\n",
    "                ddsp_model.state_dict(),\n",
    "                path.join(args.ROOT, args.NAME, \"state.pth\"),\n",
    "            )\n",
    "\n",
    "        mean_loss = 0\n",
    "        n_element = 0\n",
    "\n",
    "        audio = torch.cat([s, y], -1).reshape(-1).detach().cpu().numpy()\n",
    "\n",
    "        sf.write(\n",
    "            path.join(args.ROOT, args.NAME, f\"eval_{e:06d}.wav\"),\n",
    "            audio,\n",
    "            config[\"preprocess\"][\"sample_rate\"],\n",
    "        )\n",
    "    \n",
    "    total_loss /= len(dataloader_train)\n",
    "    losses.append(total_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Epoch')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.semilogy(losses)\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 81 \n",
      "Size of sig batch: torch.Size([1, 64000]) \n",
      "Size of sig batch: torch.Size([1, 250]) \n",
      "Size of sig batch: torch.Size([1, 250])\n"
     ]
    }
   ],
   "source": [
    "from datasets.dataset_all import Dataset, get_files\n",
    "import torch\n",
    "import yaml\n",
    "from effortless_config import Config\n",
    "\n",
    "#get_files(\"config_test.yaml\")\n",
    "\n",
    "class args(Config):\n",
    "    CONFIG = \"config_test.yaml\"\n",
    "\n",
    "args.parse_args(\"\")\n",
    "with open(args.CONFIG, \"r\") as config:\n",
    "    config = yaml.safe_load(config)\n",
    "\n",
    "out_dir = config[\"preprocess\"][\"out_dir\"]\n",
    "\n",
    "dataset = Dataset(out_dir)\n",
    "batch_size = config[\"hyperparams\"][\"batch_size\"]\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset,\n",
    "                                        batch_size,\n",
    "                                        True,\n",
    "                                        drop_last=True,\n",
    "                                        )\n",
    "\n",
    "print(\"Size of dataset:\", len(dataset), \"\\nSize of sig batch:\", next(iter(dataloader_test))['signals'].size(), \"\\nSize of sig batch:\", next(iter(dataloader_test))['pitches'].size(), \"\\nSize of sig batch:\", next(iter(dataloader_test))['loudness'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "-5.893357717073881 5.072404036155114\n",
      "tensor(-9.7821, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from models import DDSP_with_features\n",
    "import soundfile as sf\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "class args(Config):\n",
    "    CONFIG = \"config_test.yaml\"\n",
    "\n",
    "args.parse_args(\"\")\n",
    "with open(args.CONFIG, \"r\") as config:\n",
    "    config = yaml.safe_load(config)\n",
    "\n",
    "ddsp_model = DDSP_with_features(**config[\"model\"]).to(device)\n",
    "\n",
    "ddsp_model.load_state_dict(torch.load(\"runs/debug/male_speech/state.pth\"))\n",
    "\n",
    "class args(Config):\n",
    "    CONFIG = \"runs/debug/male_speech/config.yaml\"\n",
    "\n",
    "args.parse_args(\"\")\n",
    "with open(args.CONFIG, \"r\") as config:\n",
    "    config = yaml.safe_load(config)\n",
    "\n",
    "mean_loudness = config[\"data\"][\"mean_loudness\"]\n",
    "std_loudness = config[\"data\"][\"std_loudness\"]\n",
    "\n",
    "print(mean_loudness, std_loudness)\n",
    "\n",
    "batch_train = next(iter(dataloader_train))\n",
    "p_train = batch_train['pitches'].unsqueeze(-1).to(device)\n",
    "median_pitch_train = torch.median(p_train)\n",
    "\n",
    "batch = next(iter(dataloader_test))\n",
    "s = batch['signals'].to(device)\n",
    "p = batch['pitches'].unsqueeze(-1).to(device)\n",
    "l = batch['loudness'].unsqueeze(-1).to(device)\n",
    "\n",
    "l = (l - mean_loudness) / std_loudness\n",
    "median_pitch_test = torch.median(p)\n",
    "n = 12 * torch.log2(median_pitch_train/median_pitch_test)\n",
    "p = torch.pow(torch.tensor(2), n/torch.tensor(12)) * p\n",
    "\n",
    "y = ddsp_model(s, p, l).squeeze(-1)\n",
    "y = torch.cat([s, y], -1).reshape(-1).detach().cpu().numpy()\n",
    "sf.write(\"male_speech_test.wav\", y, 16000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "74e9452f59dbd6e3c7713ab707f05756754026fd1fa12e62d8045d15ad45af3c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
