{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import yaml\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from os import path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from effortless_config import Config\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from core import get_scheduler, multiscale_fft, safe_log, mean_std_loudness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from models import DDSP_signal_only, DDSP_with_features\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "class args(Config):\n",
    "    CONFIG = \"config.yaml\"\n",
    "\n",
    "args.parse_args(\"\")\n",
    "with open(args.CONFIG, \"r\") as config:\n",
    "    config = yaml.safe_load(config)\n",
    "\n",
    "ddsp_model = DDSP_with_features(**config[\"model\"]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets.dataset_signal import Dataset\n",
    "\n",
    "# dataset = Dataset(config)\n",
    "# batch_size = config[\"hyperparams\"][\"batch_size\"]\n",
    "# dataloader = torch.utils.data.DataLoader(dataset,\n",
    "#                                         batch_size,\n",
    "#                                         shuffle=False,\n",
    "#                                         drop_last=False,\n",
    "#                                         )\n",
    "\n",
    "# print(\"Size of dataset:\", len(dataset), \"\\nSize of sig batch:\", next(iter(dataloader)).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 217 \n",
      "Size of signal batch: torch.Size([16, 64000]) \n",
      "Size of pitch batch: torch.Size([16, 256]) \n",
      "Size of loudness batch: torch.Size([16, 256]) \n",
      "Size of embedding batch: torch.Size([16, 256])\n"
     ]
    }
   ],
   "source": [
    "from datasets.dataset_w_embedding import Dataset, get_files\n",
    "from effortless_config import Config\n",
    "import yaml\n",
    "import torch\n",
    "\n",
    "#get_files(\"config.yaml\")\n",
    "\n",
    "class args(Config):\n",
    "    CONFIG = \"config.yaml\"\n",
    "\n",
    "args.parse_args(\"\")\n",
    "with open(args.CONFIG, \"r\") as config:\n",
    "    config = yaml.safe_load(config)\n",
    "\n",
    "out_dir = config[\"preprocess\"][\"out_dir\"]\n",
    "\n",
    "dataset = Dataset(out_dir)\n",
    "batch_size = config[\"hyperparams\"][\"batch_size\"]\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset,\n",
    "                                        batch_size,\n",
    "                                        True,\n",
    "                                        drop_last=True,\n",
    "                                        )\n",
    "\n",
    "print(\"Size of dataset:\", len(dataset), \"\\nSize of signal batch:\", next(iter(dataloader_train))['signals'].size(), \"\\nSize of pitch batch:\", next(iter(dataloader_train))['pitches'].size(), \"\\nSize of loudness batch:\", next(iter(dataloader_train))['loudness'].size(), \"\\nSize of embedding batch:\", next(iter(dataloader_train))['embeddings'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args(Config):\n",
    "    CONFIG = \"config.yaml\"\n",
    "    NAME = \"debug/male_speech\"\n",
    "    ROOT = \"runs\"\n",
    "    STEPS = 500000\n",
    "    START_LR = 1e-3\n",
    "    STOP_LR = 1e-4\n",
    "    DECAY_OVER = 400000\n",
    "\n",
    "mean_loudness, std_loudness = mean_std_loudness(dataloader_train)\n",
    "config[\"data\"][\"mean_loudness\"] = mean_loudness\n",
    "config[\"data\"][\"std_loudness\"] = std_loudness\n",
    "\n",
    "writer = SummaryWriter(path.join(args.ROOT, args.NAME), flush_secs=20)\n",
    "\n",
    "with open(path.join(args.ROOT, args.NAME, \"config.yaml\"), \"w\") as out_config:\n",
    "    yaml.safe_dump(config, out_config)\n",
    "\n",
    "opt = torch.optim.Adam(ddsp_model.parameters(), lr=args.START_LR)\n",
    "\n",
    "schedule = get_scheduler(\n",
    "    len(dataloader_train),\n",
    "    args.START_LR,\n",
    "    args.STOP_LR,\n",
    "    args.DECAY_OVER,\n",
    ")\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "mean_loss = 0\n",
    "n_element = 0\n",
    "step = 0\n",
    "epochs = int(np.ceil(args.STEPS / len(dataloader_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses = []\n",
    "\n",
    "# def train(model, loader, optimizer):\n",
    "#     model.train()\n",
    "#     device = next(model.parameters()).device\n",
    "#     total_loss = 0\n",
    "\n",
    "#     for batch in loader:\n",
    "#         batch = batch.to(device)\n",
    "#         y = ddsp_model(batch).squeeze(-1)\n",
    "                \n",
    "#         ori_stft = multiscale_fft(\n",
    "#             batch,\n",
    "#             config[\"train\"][\"scales\"],\n",
    "#             config[\"train\"][\"overlap\"],\n",
    "#         )\n",
    "#         rec_stft = multiscale_fft(\n",
    "#             y,\n",
    "#             config[\"train\"][\"scales\"],\n",
    "#             config[\"train\"][\"overlap\"],\n",
    "#         )\n",
    "\n",
    "#         loss = 0\n",
    "#         for s_x, s_y in zip(ori_stft, rec_stft):\n",
    "#             lin_loss = (s_x - s_y).abs().mean()\n",
    "#             log_loss = (safe_log(s_x) - safe_log(s_y)).abs().mean()\n",
    "#             loss = loss + lin_loss + log_loss\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     total_loss /= len(loader)\n",
    "#     losses.append(total_loss)\n",
    "    \n",
    "#     return total_loss\n",
    "\n",
    "# for e in tqdm(range(epochs)):\n",
    "#     loss = train(ddsp_model, dataloader, opt)\n",
    "#     print(\"Epoch {} -- Loss {:3E}\".format(e, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 5368/38462 [1:22:07<8:26:21,  1.09it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Student projects\\ML-AND\\DDSP-Experiments\\ddsp_simple.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Student%20projects/ML-AND/DDSP-Experiments/ddsp_simple.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m em \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39membeddings\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Student%20projects/ML-AND/DDSP-Experiments/ddsp_simple.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m l \u001b[39m=\u001b[39m (l \u001b[39m-\u001b[39m mean_loudness) \u001b[39m/\u001b[39m std_loudness\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Student%20projects/ML-AND/DDSP-Experiments/ddsp_simple.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m y \u001b[39m=\u001b[39m ddsp_model(s, p, l, em)\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Student%20projects/ML-AND/DDSP-Experiments/ddsp_simple.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m ori_stft \u001b[39m=\u001b[39m multiscale_fft(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Student%20projects/ML-AND/DDSP-Experiments/ddsp_simple.ipynb#W6sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     s,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Student%20projects/ML-AND/DDSP-Experiments/ddsp_simple.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     config[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mscales\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Student%20projects/ML-AND/DDSP-Experiments/ddsp_simple.ipynb#W6sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     config[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39moverlap\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Student%20projects/ML-AND/DDSP-Experiments/ddsp_simple.ipynb#W6sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Student%20projects/ML-AND/DDSP-Experiments/ddsp_simple.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m rec_stft \u001b[39m=\u001b[39m multiscale_fft(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Student%20projects/ML-AND/DDSP-Experiments/ddsp_simple.ipynb#W6sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     y,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Student%20projects/ML-AND/DDSP-Experiments/ddsp_simple.ipynb#W6sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     config[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mscales\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Student%20projects/ML-AND/DDSP-Experiments/ddsp_simple.ipynb#W6sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     config[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39moverlap\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Student%20projects/ML-AND/DDSP-Experiments/ddsp_simple.ipynb#W6sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Me-Lab_Chimaera\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Student projects\\ML-AND\\DDSP-Experiments\\models.py:229\u001b[0m, in \u001b[0;36mDDSP_with_features.forward\u001b[1;34m(self, signal, pitch, loudness, embedding)\u001b[0m\n\u001b[0;32m    226\u001b[0m harmonic \u001b[39m=\u001b[39m harmonic_synth(pitch, amplitudes, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_rate)\n\u001b[0;32m    228\u001b[0m impulse \u001b[39m=\u001b[39m amp_to_impulse_response(param_noise, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock_size)\n\u001b[1;32m--> 229\u001b[0m noise \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrand(\n\u001b[0;32m    230\u001b[0m     impulse\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m],\n\u001b[0;32m    231\u001b[0m     impulse\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m],\n\u001b[0;32m    232\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblock_size,\n\u001b[0;32m    233\u001b[0m )\u001b[39m.\u001b[39mto(impulse) \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    235\u001b[0m noise \u001b[39m=\u001b[39m fft_convolve(noise, impulse)\u001b[39m.\u001b[39mcontiguous()\n\u001b[0;32m    236\u001b[0m noise \u001b[39m=\u001b[39m noise\u001b[39m.\u001b[39mreshape(noise\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "losses = []\n",
    "\n",
    "for e in tqdm(range(epochs)):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader_train:\n",
    "        s = batch['signals'].to(device)\n",
    "        p = batch['pitches'].unsqueeze(-1).to(device)\n",
    "        l = batch['loudness'].unsqueeze(-1).to(device)\n",
    "        em = batch['embeddings'].unsqueeze(-1).to(device)\n",
    "\n",
    "        l = (l - mean_loudness) / std_loudness\n",
    "\n",
    "        y = ddsp_model(s, p, l, em).squeeze(-1)\n",
    "\n",
    "        ori_stft = multiscale_fft(\n",
    "            s,\n",
    "            config[\"train\"][\"scales\"],\n",
    "            config[\"train\"][\"overlap\"],\n",
    "        )\n",
    "        rec_stft = multiscale_fft(\n",
    "            y,\n",
    "            config[\"train\"][\"scales\"],\n",
    "            config[\"train\"][\"overlap\"],\n",
    "        )\n",
    "\n",
    "        loss = 0\n",
    "        for s_x, s_y in zip(ori_stft, rec_stft):\n",
    "            lin_loss = (s_x - s_y).abs().mean()\n",
    "            log_loss = (safe_log(s_x) - safe_log(s_y)).abs().mean()\n",
    "            loss = loss + lin_loss + log_loss\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        writer.add_scalar(\"loss\", loss.item(), step)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "        n_element += 1\n",
    "        mean_loss += (loss.item() - mean_loss) / n_element\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if not e % 10:\n",
    "        writer.add_scalar(\"lr\", schedule(e), e)\n",
    "        writer.add_scalar(\"reverb_decay\", ddsp_model.reverb.decay.item(), e)\n",
    "        writer.add_scalar(\"reverb_wet\", ddsp_model.reverb.wet.item(), e)\n",
    "        # scheduler.step()\n",
    "        if mean_loss < best_loss:\n",
    "            best_loss = mean_loss\n",
    "            torch.save(\n",
    "                ddsp_model.state_dict(),\n",
    "                path.join(args.ROOT, args.NAME, \"state.pth\"),\n",
    "            )\n",
    "\n",
    "        mean_loss = 0\n",
    "        n_element = 0\n",
    "\n",
    "        audio = torch.cat([s, y], -1).reshape(-1).detach().cpu().numpy()\n",
    "\n",
    "        sf.write(\n",
    "            path.join(args.ROOT, args.NAME, f\"eval_{e:06d}.wav\"),\n",
    "            audio,\n",
    "            config[\"preprocess\"][\"sample_rate\"],\n",
    "        )\n",
    "    \n",
    "    total_loss /= len(dataloader_train)\n",
    "    losses.append(total_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Epoch')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.semilogy(losses)\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 81 \n",
      "Size of sig batch: torch.Size([1, 64000]) \n",
      "Size of sig batch: torch.Size([1, 250]) \n",
      "Size of sig batch: torch.Size([1, 250])\n"
     ]
    }
   ],
   "source": [
    "from datasets.dataset_w_embedding import Dataset, get_files\n",
    "import torch\n",
    "import yaml\n",
    "from effortless_config import Config\n",
    "\n",
    "#get_files(\"config_test.yaml\")\n",
    "\n",
    "class args(Config):\n",
    "    CONFIG = \"config_test.yaml\"\n",
    "\n",
    "args.parse_args(\"\")\n",
    "with open(args.CONFIG, \"r\") as config:\n",
    "    config = yaml.safe_load(config)\n",
    "\n",
    "out_dir = config[\"preprocess\"][\"out_dir\"]\n",
    "\n",
    "dataset = Dataset(out_dir)\n",
    "batch_size = config[\"hyperparams\"][\"batch_size\"]\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset,\n",
    "                                        batch_size,\n",
    "                                        True,\n",
    "                                        drop_last=True,\n",
    "                                        )\n",
    "\n",
    "print(\"Size of dataset:\", len(dataset), \"\\nSize of sig batch:\", next(iter(dataloader_test))['signals'].size(), \"\\nSize of sig batch:\", next(iter(dataloader_test))['pitches'].size(), \"\\nSize of sig batch:\", next(iter(dataloader_test))['loudness'].size(), \"\\nSize of sig batch:\", next(iter(dataloader_test))['embeddings'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "-5.893357717073881 5.072404036155114\n",
      "tensor(-9.7821, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from models import DDSP_with_features\n",
    "import soundfile as sf\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "class args(Config):\n",
    "    CONFIG = \"config_test.yaml\"\n",
    "\n",
    "args.parse_args(\"\")\n",
    "with open(args.CONFIG, \"r\") as config:\n",
    "    config = yaml.safe_load(config)\n",
    "\n",
    "ddsp_model = DDSP_with_features(**config[\"model\"]).to(device)\n",
    "\n",
    "ddsp_model.load_state_dict(torch.load(\"runs/debug/male_speech/state.pth\"))\n",
    "\n",
    "class args(Config):\n",
    "    CONFIG = \"runs/debug/male_speech/config.yaml\"\n",
    "\n",
    "args.parse_args(\"\")\n",
    "with open(args.CONFIG, \"r\") as config:\n",
    "    config = yaml.safe_load(config)\n",
    "\n",
    "mean_loudness = config[\"data\"][\"mean_loudness\"]\n",
    "std_loudness = config[\"data\"][\"std_loudness\"]\n",
    "\n",
    "print(mean_loudness, std_loudness)\n",
    "\n",
    "batch_train = next(iter(dataloader_train))\n",
    "p_train = batch_train['pitches'].unsqueeze(-1).to(device)\n",
    "median_pitch_train = torch.median(p_train)\n",
    "\n",
    "batch = next(iter(dataloader_test))\n",
    "s = batch['signals'].to(device)\n",
    "p = batch['pitches'].unsqueeze(-1).to(device)\n",
    "l = batch['loudness'].unsqueeze(-1).to(device)\n",
    "\n",
    "l = (l - mean_loudness) / std_loudness\n",
    "median_pitch_test = torch.median(p)\n",
    "n = 12 * torch.log2(median_pitch_train/median_pitch_test)\n",
    "p = torch.pow(torch.tensor(2), n/torch.tensor(12)) * p\n",
    "\n",
    "y = ddsp_model(s, p, l).squeeze(-1)\n",
    "y = torch.cat([s, y], -1).reshape(-1).detach().cpu().numpy()\n",
    "sf.write(\"male_speech_test.wav\", y, 16000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d14eec7a70b15df254ac1ea217638b23a4f413adccbe2ef552ea76f41ff5b606"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
